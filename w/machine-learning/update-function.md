In [gradient descent](gradient-descent), the update function is repeatedly applied to find the parameters θ that minimzes the [cost function](cost-function) J(θ).

The update function itself is:

![update-function](png/update-function)

α is called the learning rate

We implement it using the update rule:

![update-rule](png/update-rule)

See [derivation of the update rule](update-rule-derivation)
